# ğŸ¤– LangGraph Research Assistant

A **multi-agent RAG (Retrieval-Augmented Generation)** system built using **LangGraph**, **LangChain**, and **LangSmith**, designed to simulate how intelligent AI agents retrieve, reason, and synthesize knowledge collaboratively.

> This project demonstrates a full local RAG pipeline â€” **no external OpenAI API required** â€” with FAISS vector search, multi-agent orchestration, LangSmith tracing, and Docker deployment.

---

## ğŸš€ Features

- ğŸ§  **LangGraph Multi-Agent Orchestration**  
  Modular design with planner, retriever, and synthesizer agents.
  
- ğŸ“š **RAG (Retrieval-Augmented Generation)**  
  Retrieves relevant documents using FAISS before synthesizing answers.

- ğŸ” **FAISS Vectorstore**  
  Handles efficient document indexing and semantic similarity search.

- ğŸ“ˆ **LangSmith Integration**  
  Traces and monitors agent events and execution flow.

- ğŸ³ **Dockerized Setup**  
  Fully containerized â€” deploy anywhere.

- ğŸ§© **Local LLM Fallback**  
  When OpenAI API is unavailable, a deterministic synthesizer generates context-based answers locally.

---

## ğŸ—ï¸ Architecture Overview

```
User Query
â”‚
â–¼
[ Planner Agent ]
â”‚
â–¼
[ Retriever Agent ] â†’ Fetches top-k context documents (via FAISS)
â”‚
â–¼
[ Synthesizer Agent ] â†’ Generates context-based final answer
â”‚
â–¼
Response returned to FastAPI endpoint (/run)
```

---

## ğŸ§ª Tech Stack

| Layer | Tool / Library | Description |
|-------|----------------|--------------|
| Framework | **FastAPI** | Backend API layer |
| Orchestration | **LangGraph** | Multi-agent workflow |
| Retrieval | **FAISS + LangChain** | Vector similarity search |
| Embeddings | **Local Sentence Embeddings / OpenAI (optional)** | Document encoding |
| Monitoring | **LangSmith** | Event tracing and pipeline visualization |
| Containerization | **Docker** | Build and deployment |

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/GEEGEEGOOGOO/LangGraph-Research-Assistant.git
cd LangGraph-Research-Assistant
```

### 2ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv .venv
.\.venv\Scripts\activate   # Windows
source .venv/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Build Vectorstore (FAISS)

```bash
python -m app.vectorstore
```

Expected output:

```
[SUCCESS] FAISS index saved at ./data/faiss_index
```

### 5ï¸âƒ£ Run the FastAPI Server

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

When you see:

```
INFO: Application startup complete.
```

your server is running at ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§  Example Queries (Swagger UI)

**Endpoint:** `POST /run`

Example request:

```json
{
  "query": "Explain how RAG works"
}
```

Example response:

```json
{
  "planner_action": "retrieve",
  "retrieved_docs": [
    "Retrieval-Augmented Generation (RAG) adds context to LLMs using vector search.",
    "Model Context Protocol (MCP) defines how models share tools and resources.",
    "LangGraph supports multi-agent reasoning and orchestration."
  ],
  "final_answer": "LOCAL SYNTHESIS for query: 'Explain how RAG works' ..."
}
```

---

## ğŸ§© Project Structure

```
LangGraph_Research_Assistant_v2/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ planner.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ synthesizer.py
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â””â”€â”€ embeddings.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ faiss_index/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”œâ”€â”€ test_synthesizer.py
â”‚   â””â”€â”€ test_planner.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ³ Docker Deployment

Build and run your containerized assistant:

```bash
docker build -t langgraph-assistant:latest .
docker run -p 8000:8000 langgraph-assistant:latest
```

---

## ğŸ§© Demo

### ğŸ”¹ Swagger UI Screenshot

![Swagger UI](assets/SwaggerU_ResponseI.png)

### ğŸ”¹ Terminal Output (FAISS Index)

![FAISS Output](assets/Demo_Query_Response.png)

### ğŸ”¹ Server Running

![Server status](assets/Demo_Server_Running.png)

### ğŸ”¹ Docker File

![Docker Working](assets/Docker_SS.png)

### ğŸ”¹ API Demo GIF

![Demo](assets/chrome_ELVc46T9a7.gif)

---

## ğŸ§  Key Learning Outcomes

* Implemented **LangGraph** for multi-agent reasoning.
* Designed a **Retrieval-Augmented Generation** pipeline.
* Integrated **LangSmith** for workflow tracing.
* Built deterministic **local fallback synthesis** (no API required).
* Deployed the entire workflow using **Docker**.

---

## ğŸ¯ Ideal For

* **AIML Internship applications**
* **LangGraph & RAG architecture understanding**
* **Demonstrating full AI pipeline engineering**

---

## ğŸ“„ License

MIT License Â© 2025 â€” Created by **[Shashank Kumar]**

---

## ğŸ“¬ Contact

Feel free to reach out for questions or collaboration opportunities!

- **GitHub:** [@GEEGEEGOOGOO](https://github.com/GEEGEEGOOGOO)
- **Email:** shashank181002@gmail.com
- **LinkedIn:** [MyProfile](https://linkedin.com/in/shashank1810)
